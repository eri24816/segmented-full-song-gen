{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "\n",
    "# Simple CNN denoiser\n",
    "class SimpleDenoiser(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.in_layer = nn.Sequential(\n",
    "            nn.Conv2d(2, 32, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.down1 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.down2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Linear(256 * 7 * 7, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 256 * 7 * 7),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 128, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.up2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 64, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.out_layer = nn.Sequential(\n",
    "            nn.Conv2d(64, 1, 3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        t = t.view(-1, 1, 1, 1)\n",
    "        t = t.repeat(1, 1, x.shape[2], x.shape[3])\n",
    "        x = torch.cat([x, t], dim=1)\n",
    "        d0 = self.in_layer(x)\n",
    "        d1 = self.down1(d0)\n",
    "        d2 = self.down2(d1)\n",
    "        x = self.bottleneck(d2.view(d2.shape[0], -1))\n",
    "        x = x.view(x.shape[0], 256, 7, 7)\n",
    "        u1 = self.up1(x)\n",
    "        u2 = self.up2(u1 + d1)\n",
    "        return self.out_layer(u2 + d0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Generic, TypeVar, Generator\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def is_notimplemented(func):\n",
    "    return hasattr(func, \"__notimplemented__\")\n",
    "\n",
    "\n",
    "def notimplemented(func):\n",
    "    func.__notimplemented__ = True\n",
    "    return func\n",
    "\n",
    "\n",
    "BackwardParams = TypeVar(\"BackwardParams\", bound=dict)\n",
    "\n",
    "\n",
    "class DiffusionModel(nn.Module, ABC, Generic[BackwardParams]):\n",
    "    \"\"\"\n",
    "    An abstract class for diffusion models.\n",
    "    Defines the core interface that all diffusion models should implement.\n",
    "\n",
    "    For any function that have argument `t`, t ranges from 1 to num_steps. (It's a convention for diffusion models)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_steps: int):\n",
    "        super().__init__()\n",
    "        self.num_steps = num_steps\n",
    "        self.device = torch.device(\"cpu\")\n",
    "\n",
    "    def to(self, device: torch.device):\n",
    "        self.device = device\n",
    "        return super().to(device)\n",
    "\n",
    "    def sample_t(self, batch_size: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sample timesteps for training. Uniformly sample from [0, num_steps) by default.\n",
    "        Override this method to sample from a different distribution.\n",
    "\n",
    "        Args:\n",
    "            batch_size: Number of timesteps to sample\n",
    "            device: Device to put the sampled timesteps on\n",
    "\n",
    "        Returns:\n",
    "            Tensor of timesteps\n",
    "        \"\"\"\n",
    "        return torch.randint(1, self.num_steps + 1, (batch_size,), device=self.device, dtype=torch.long)\n",
    "\n",
    "    def get_batch_t(self, t: int, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Get the batch of timesteps with shape (x.shape[0], 1, 1, ..., 1) so x and the returned t can be broadcasted together.\n",
    "        \"\"\"\n",
    "        return torch.full((x.shape[0],) + (1,) * (len(x.shape) - 1), t, device=self.device, dtype=torch.long)\n",
    "\n",
    "    @abstractmethod\n",
    "    def sample_x_T(self, shape: tuple[int, ...]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sample from p(x_T)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward_one_step(self, x_t: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward one step of the diffusion process.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward_from_x0(self, x_0: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sample from q(x_t | x_0) - the forward diffusion process.\n",
    "\n",
    "        Args:\n",
    "            x_0: Initial data\n",
    "            t: Timesteps\n",
    "\n",
    "        Returns:\n",
    "            The noised data x_t\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward_process(self, x_0: torch.Tensor):\n",
    "        \"\"\"\n",
    "        A generator that yields (t, x_t) with t from 0 to num_steps.\n",
    "        \"\"\"\n",
    "        x_t = x_0\n",
    "        for t in range(self.num_steps):\n",
    "            yield t, x_t\n",
    "            x_t = self.forward_from_x0(x_t, self.get_batch_t(t, x_t))\n",
    "\n",
    "        yield self.num_steps, x_t\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward_one_step(self, x_t: torch.Tensor, t: int, **kwargs: BackwardParams) -> tuple[torch.Tensor, dict]:\n",
    "        \"\"\"\n",
    "        Sample from p(x_{t-1} | x_t).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @notimplemented\n",
    "    def backward(self, x_t: torch.Tensor, t2: int, t1: int, **kwargs: BackwardParams) -> tuple[torch.Tensor, dict]:\n",
    "        \"\"\"\n",
    "        Sample from  p(x_{t1} | x_{t2}).\n",
    "\n",
    "        Optionally implement this method for a more efficient backward pass across multiple timesteps.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Backward is not implemented\")\n",
    "\n",
    "    def _backward(self, x_t: torch.Tensor, t2: int, t1: int, **kwargs: BackwardParams) -> tuple[torch.Tensor, dict]:\n",
    "        \"\"\"\n",
    "        Sample from p(x_{t1} | x_{t2}). If backward is not implemented, this method falls back\n",
    "        to multiple calls to backward_one_step.\n",
    "        \"\"\"\n",
    "        if is_notimplemented(self.backward):\n",
    "            for t in range(t2, t1, -1):\n",
    "                x_t, info = self.backward_one_step(x_t, t, **kwargs)\n",
    "            return x_t, info\n",
    "        else:\n",
    "            return self.backward(x_t, t2, t1, **kwargs)\n",
    "\n",
    "    def backward_process(\n",
    "        self, x_t: torch.Tensor, **kwargs: BackwardParams\n",
    "    ) -> Generator[tuple[int, torch.Tensor, dict], None, None]:\n",
    "        \"\"\"\n",
    "        A generator that yields (t, x_{t-1}, info) with t from num_steps to 0.\n",
    "        \"\"\"\n",
    "        info = {}\n",
    "        for t in range(self.num_steps, 0, -1):\n",
    "            yield t, x_t, info\n",
    "            x_t, info = self.backward_one_step(x_t, t, **kwargs)\n",
    "        yield 0, x_t, info\n",
    "\n",
    "    def sample(\n",
    "        self, shape: tuple[int, ...], steps_to_return: int | list[int] = 0, **kwargs: BackwardParams\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        - Sample from p(x_0) if steps_to_return is 0 (default). Return shape is the same as the shape argument.\n",
    "        - Sample from p(x_t) if steps_to_return is specified to t. Return shape is the same as the shape argument.\n",
    "        - Return intermediate steps if steps_to_return is a list of timesteps. Return shape is (len(steps_to_return), *shape).\n",
    "\n",
    "        Args:\n",
    "            shape: Shape of samples to generate\n",
    "            steps_to_return: Specify to return intermediate steps\n",
    "            **kwargs: Additional sampling arguments\n",
    "\n",
    "        Returns:\n",
    "            Generated samples.\n",
    "        \"\"\"\n",
    "\n",
    "        return_one_step = isinstance(steps_to_return, int)\n",
    "\n",
    "        if isinstance(steps_to_return, int):\n",
    "            steps_to_return = [steps_to_return]\n",
    "\n",
    "        x_t = self.sample_x_T(shape)\n",
    "\n",
    "        last_t = self.num_steps\n",
    "\n",
    "        result = []\n",
    "        for t in steps_to_return:\n",
    "            if t != last_t:\n",
    "                x_t, info = self._backward(x_t, last_t, t, **kwargs)\n",
    "                last_t = t\n",
    "            result.append(x_t)\n",
    "\n",
    "        if return_one_step:\n",
    "            return result[0]\n",
    "        else:\n",
    "            return torch.stack(result, dim=0)\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x0: torch.Tensor, **kwargs) -> dict:\n",
    "        \"\"\"\n",
    "        Training forward pass. Return a dictionary containing loss and other metrics.\n",
    "\n",
    "        Args:\n",
    "            x0: Input data\n",
    "            **kwargs: Additional forward pass arguments\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with required 'loss' field and optional additional metrics\n",
    "        \"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Generator, Tuple, Dict, TypeVar\n",
    "\n",
    "\n",
    "class GaussianDDPM(DiffusionModel[Dict]):\n",
    "    def __init__(self, num_steps: int, denoiser: nn.Module, beta_start: float = 1e-4, beta_end: float = 0.02):\n",
    "        \"\"\"\n",
    "        Gaussian Diffusion Model without an explicit scheduler.\n",
    "        Stores alpha, alpha_bar, and beta tensors in the model.\n",
    "\n",
    "        Args:\n",
    "            num_steps: Number of diffusion steps.\n",
    "            denoiser: The neural network model used for denoising.\n",
    "            beta_start: Start value for beta schedule.\n",
    "            beta_end: End value for beta schedule.\n",
    "        \"\"\"\n",
    "        super().__init__(num_steps)\n",
    "        self.denoiser = denoiser.to(self.device)\n",
    "\n",
    "        # Define beta schedule (linear schedule)\n",
    "        beta = torch.linspace(beta_start, beta_end, num_steps)\n",
    "\n",
    "        # Compute alpha and alpha_bar\n",
    "        alpha = 1.0 - beta\n",
    "        alpha_bar = torch.cumprod(alpha, dim=0)\n",
    "\n",
    "        # please type checker\n",
    "        self.beta: torch.Tensor\n",
    "        self.alpha: torch.Tensor\n",
    "        self.alpha_bar: torch.Tensor\n",
    "\n",
    "        # pad left so the index is 1-based\n",
    "        beta = torch.cat([torch.zeros(1), beta], dim=0)\n",
    "        alpha = torch.cat([torch.ones(1), alpha], dim=0)\n",
    "        alpha_bar = torch.cat([torch.ones(1), alpha_bar], dim=0)\n",
    "\n",
    "        # Store tensors as buffers (moved with model but not updated)\n",
    "        self.register_buffer(\"beta\", beta)\n",
    "        self.register_buffer(\"alpha\", alpha)\n",
    "        self.register_buffer(\"alpha_bar\", alpha_bar)\n",
    "\n",
    "    def extract(self, source: torch.Tensor, t: torch.Tensor, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Extract the values from the source tensor at the given timestep.\n",
    "        \"\"\"\n",
    "        return source[t].view([t.shape[0]] + [1] * (len(x.shape) - 1)).to(x.device)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def sample_x_T(self, shape: Tuple[int, ...]) -> torch.Tensor:\n",
    "        \"\"\"Sample from p(x_T), which is standard Gaussian noise.\"\"\"\n",
    "        return torch.randn(shape, device=self.device)\n",
    "\n",
    "    def forward_from_x0(self, x_0: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward diffusion process q(x_t | x_0).\n",
    "        This adds noise to x_0 according to the diffusion schedule.\n",
    "\n",
    "        Args:\n",
    "            x_0: Initial data (clean input).\n",
    "            t: Timesteps.\n",
    "\n",
    "        Returns:\n",
    "            x_t: Noised data at time t.\n",
    "        \"\"\"\n",
    "        assert (t >= 1).all() and (t <= self.num_steps).all(), f\"t must be between 1 and {self.num_steps}, got {t}\"\n",
    "        alpha_bar_t = self.alpha_bar[t].view(-1, 1, 1, 1)  # Reshape for broadcasting\n",
    "        noise = torch.randn_like(x_0)\n",
    "        x_t = torch.sqrt(alpha_bar_t) * x_0 + torch.sqrt(1 - alpha_bar_t) * noise\n",
    "        return x_t, noise\n",
    "\n",
    "    def forward_one_step(self, x_t: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward one step in the diffusion process (q(x_{t+1} | x_t)).\n",
    "        This is usually not explicitly needed in standard DDPM formulations.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Not needed for standard DDPM\")\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def backward_one_step(self, x_t: torch.Tensor, t: int, **kwargs) -> Tuple[torch.Tensor, Dict]:\n",
    "        \"\"\"\n",
    "        Reverse diffusion step p(x_{t-1} | x_t) using learned denoiser.\n",
    "\n",
    "        Args:\n",
    "            x_t: The current noised sample at timestep t.\n",
    "            t: The current timestep.\n",
    "\n",
    "        Returns:\n",
    "            x_{t-1}: The predicted less noisy sample.\n",
    "            info: Dictionary containing intermediate values.\n",
    "        \"\"\"\n",
    "\n",
    "        assert t >= 1 and t <= self.num_steps, f\"t must be between 1 and {self.num_steps}, got {t}\"\n",
    "        t_tensor = torch.full((x_t.shape[0],), t, device=self.device, dtype=torch.long)\n",
    "\n",
    "        # Predict x_0 from x_t using the denoiser\n",
    "        eps_pred = self.denoiser(x_t, t_tensor.float() / self.num_steps)\n",
    "\n",
    "        # Retrieve precomputed values\n",
    "        alpha_bar = self.extract(self.alpha_bar, t_tensor, x_t)\n",
    "        beta = self.extract(self.beta, t_tensor, x_t)\n",
    "\n",
    "        alpha = self.extract(self.alpha, t_tensor, x_t)\n",
    "        alpha_bar_last = self.extract(self.alpha_bar, t_tensor-1, x_t)\n",
    "\n",
    "        # Compute the mean of the reverse step\n",
    "        mu_t = (1 / torch.sqrt(alpha)) * (x_t - ((1-alpha) / torch.sqrt(1 - alpha_bar)) * eps_pred)\n",
    "\n",
    "        # Add noise for stochastic sampling (except at t=0)\n",
    "        if t > 0:\n",
    "            noise = torch.randn_like(x_t) * torch.sqrt(beta)\n",
    "            x_t_prev = mu_t + noise\n",
    "        else:\n",
    "            x_t_prev = mu_t  # No noise at final step\n",
    "\n",
    "        return x_t_prev, {}\n",
    "\n",
    "    def get_eps(self, x0: torch.Tensor, xt: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        alpha_bar = self.extract(self.alpha_bar, t, xt)\n",
    "        return (xt - torch.sqrt(alpha_bar) * x0) / torch.sqrt(1 - alpha_bar)\n",
    "\n",
    "    def forward(self, x0: torch.Tensor, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Training forward pass.\n",
    "\n",
    "        1. Sample a timestep `t`.\n",
    "        2. Generate `x_t` using the forward diffusion process.\n",
    "        3. Predict `x_0` from `x_t` using the denoiser.\n",
    "        4. Compute loss between `x_0` and the predicted `x_0`.\n",
    "\n",
    "        Args:\n",
    "            x0: Input data (clean images or signals).\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing loss and optional metrics.\n",
    "        \"\"\"\n",
    "        batch_size = x0.shape[0]\n",
    "        t = self.sample_t(batch_size)\n",
    "\n",
    "        xt, noise = self.forward_from_x0(x0, t)\n",
    "\n",
    "        eps = self.get_eps(x0, xt, t)\n",
    "\n",
    "        assert (noise-eps).abs().max() < 1e-5, \"noise and eps are not the same\"\n",
    "\n",
    "        eps_pred = self.denoiser(xt, t.float() / self.num_steps)\n",
    "\n",
    "        loss = F.mse_loss(eps_pred, eps)\n",
    "\n",
    "        return {\"loss\": loss, 't': t, 'eps': eps, 'eps_pred': eps_pred}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = torchvision.datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Initialize model\n",
    "denoiser = SimpleDenoiser()\n",
    "diffusion = GaussianDDPM(num_steps=500, denoiser=denoiser)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "diffusion.to(device)\n",
    "\n",
    "optimizer = optim.Adam(denoiser.parameters(), lr=5e-4)\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        x0, _ = batch\n",
    "        x0 = x0.to(device)\n",
    "\n",
    "        x0 = x0 * 2 - 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_dict = diffusion.forward(x0)\n",
    "        loss = loss_dict[\"loss\"]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / 100}\")\n",
    "            total_loss = 0\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_diffusion = diffusion\n",
    "diffusion = GaussianDDPM(num_steps=500, denoiser=denoiser)\n",
    "diffusion.load_state_dict(old_diffusion.state_dict())\n",
    "del old_diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, title=\"Generated Images\"):\n",
    "    grid = make_grid(images, nrow=8, normalize=True, value_range=(-1, 1))\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy(), vmin=-1, vmax=1)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Generate images by denoising from x_T\n",
    "diffusion.eval()\n",
    "num_samples = 16\n",
    "xt = diffusion.sample_x_T((num_samples, 1, 28, 28))\n",
    "\n",
    "for t_tensor in range(500, 0, -1):  # Reverse diffusion steps\n",
    "    xt, a = diffusion.backward_one_step(xt, t_tensor)\n",
    "\n",
    "show_images(xt, \"Generated MNIST Samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(diffusion.sample(xt.shape, [500, 400, 300, 200, 100, 50, 20, 10, 5, 1])[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion.sample((1,1,28,28)).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
